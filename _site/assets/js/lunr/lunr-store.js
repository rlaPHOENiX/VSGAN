var store = [{
        "title": "Introduction",
        "excerpt":"VSGAN is a Single Image Super-Resolution Generative Adversarial Network (GAN) which uses the VapourSynth processing framework to handle input and output image data.   Note: The GAN Architecture is exactly that of ESRGAN by xinntao. All accomplishments of ESRGAN will also be achieved with VSGAN.   Warning: Depending on the use-case, performance may not match the original ESRGAN, however, it won’t be drastically slower.   Quick Terminology Gloss-over                  Term       Meaning                       Single Image       Using the data from one image for one output image.                 Super-Resolution (SR)       Known otherwise as Upscaling/Upconverting/Resizing.                 Generative Adversarial Network (GAN)       Adversarial which a Generator (G) network generates data, and a Discriminator (D) tries to detect if the generated image is perceived as fake.                 Ground Truth (GT)       The original high resolution image/data. Also known as HR (High-resolution).           Introduction to ESRGAN   ESRGAN: Enhanced super-resolution generative adversarial network. First place winner in PIRM2018-SR competition (Region 3) with the best perceptual index. The paper is accepted to ECCV2018 PIRM Workshop.   It’s an improvement of SRGAN in three aspects:      Adopt a deeper model using Residual-in-Residual Dense Block (RRDB) without batch normalization layers.   Employ Relativistic average GAN instead of the vanilla GAN.   Improve the perceptual loss by using the features before activation.      Authors: Wang, X., Yu, K., Wu, S., Gu, J., Liu, Y., Dong, C., Qiao, Y., &amp; Loy, C. (2018)    Comparisons                                                                   Super-resolution Architectures Comparison between Bicubic (Algorithm), SRCNN, EDSR, RCAN, EnhanceNet, SRGAN, and ESRGAN. Source: ESRGAN   Training Results                  Method       Training dataset       Set5       Set14       BSD100       Urban100       Manga109                       SRCNN       291       30.48/0.8628       27.50/0.7513       26.90/0.7101       24.52/0.7221       27.58/0.8555                 EDSR       DIV2K       32.46/0.8968       28.80/0.7876       27.71/0.7420       26.64/0.8033       31.02/0.9148                 RCAN       DIV2K       32.63/0.9002       28.87/0.7889       27.77/0.7436       26.82/ 0.8087       31.22/ 0.9173                 RRDB (ESRGAN)       DF2K       32.73/0.9011       28.99/0.7917       27.85/0.7455       27.03/0.8153       31.66/0.9196               The RRDB PSNR oriented model trained with DF2K dataset (DIV2K &amp; Flickr2K,    proposed in EDSR) compared against other Super-resolution Architectures. Best result in each validation marked bold. Source: ESRGAN   More information   If you wish to learn more about training, stats, or learn it’s issues, then check out the ESRGAN README.   Benefits of VSGAN over ESRGAN   Since VapourSynth is primed to pass a frame sequence to a script; this could be considered a Video Super-Resolution Network that doesn’t take advantage of neighbouring frame data. This allows you to pass hundreds of thousands of frames from video files through ESRGAN from an input video file, removing the need to extract the frames out of a video which takes a lot of time, processing power, and file space.   VapourSynth provides an extreme amount of image processing functionality for pretty much anything you can imagine to do with an image programmatically. For example, pre and post filtering of the frames with scaling, cropping, flipping, rotating, denoising, color augmentations, comparisons, and more. Not to mention advanced possibilities like fixing the Chroma location positioning (aka chroma bleed) or chroma size (chroma droop), Chroma-channel noise, Dot-crawl, Rainbowing, and more!   You can see a database of readily-available scripts and plugins for all kinds of purposes on VSDB. For help with VapourSynth head to Doom9 or VideoHelp.   Example VSGAN output                             Before (left): 720x480 resolution frame from American Dad S01E01 (USA R1 NTSC DVD). After (right): A private ESRGAN 4x scale model       using VSGAN on the before frame. This model was trained to fix inaccuracies in the DVD's color, remove Halo'ing/Glow,       and remove Chroma Droop. The result is a very crisp output for a show originally animated in SD. It's using VapourSynth's       core.std.StackHorizontal to provide a before and after comparison, with core.text.Text for the Before and After labels.      ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/",
        "teaser": null
      },{
        "title": "Installation",
        "excerpt":"Hardware Requirements   GPU: NVIDIA GPU that has support for CUDA 9.2+. A CUDA Compute Capability score of 6 or higher is recommended, and a score &lt;= 2 will be incredibly slow, if it works at all.  CPU: There’s no specific requirement for the CPU, just try and use a CPU that won’t bottle-neck your GPU. The CPU will be used, but not much.   No Supported GPU? If you don’t mind waiting minutes or even hours per frame, then you can use your CPU instead of the GPU. Just note that it’s not our responsibility if over-use of your CPU, lower it’s life span, or even get it killed. Expect constantly high CPU Usage and Temperatures causing even your mouse to lag.   Software Dependencies      Python 3.5+ and Python PIP.   VapourSynth latest recommended or R40+. Ensure the Python version you have installed is supported by the version of VapourSynth that you will use. The supported Python versions may differ per OS.   PyTorch latest recommended or 1.6.0+. Ensure you install PyTorch with CUDA support if you plan to use your GPU.   NVIDIA CUDA latest recommended or 9.2+. Ensure the version is supported by the version of PyTorch that you will use.   Tip: Ensure Python is added to PATH when installed (Tick Add Python X.X to PATH in Customize Installation mode) for an optimal and swift installation experience.   Tip: When installing VapourSynth, ensure you follow all the instructions on their official website for your OS. There are important instructions specific to each OS.   Installing VSGAN   After all that dependency installation you’re finally going to get redemption for your efforts! :D   pip install vsgan  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/installation/",
        "teaser": null
      },{
        "title": "Updating",
        "excerpt":"It’s as simple as pip install --upgrade vsgan   :O  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/updating/",
        "teaser": null
      },{
        "title": "Building",
        "excerpt":"This project is firmly requiring the use of Python PIP with PEP 517 support. This means you need pip &gt;= 19.0.   Considering version 19.0 released on the 22nd of January 2019, it isn’t much of an ask in my opinion, when you end up with an overall much smoother build experience.   Building distribution files   pip install build git clone https://github.com/rlaPHOENiX/vsgan &amp;&amp; cd vsgan python -m build   To install the built distribution files, install the .whl file available in /dist, e.g. pip install dist/*.whl   Installing from source   git clone https://github.com/rlaPHOENiX/vsgan &amp;&amp; cd vsgan pip install .   ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/building/",
        "teaser": null
      },{
        "title": "Quick Start",
        "excerpt":"Here’s a quick example of VSGAN usage. Further Information on the classes and functions used can be found in the Functions documentation.   import vapoursynth as vs  from vsgan import VSGAN  # ...  # PyTorch device, e.g. \"cpu\", \"cuda\", \"cuda:0\", \"cuda:1\", 0, 1, ..., e.t.c device = \"cuda\"  # ESRGAN model file, see http://localhost:4000/models/ # tip: prepend path with r\" ... \" if path separaters use \\ and not / model = r\"C:\\Users\\PHOENiX\\Documents\\ESRGAN Models\\PSNR_x4_DB.pth\"  # 1. Create a VSGAN instance, which creates a PyTorch device instance vsgan = VSGAN(device)  # 2. Load an ESRGAN model into the VSGAN instance # tip: You can run load_model() at any point to change the model vsgan.load_model(model)  # 3. Convert the clip to RGB24 as ESRGAN can only work with linear RGB data clip = core.resize.Point(clip, format=vs.RGB24)  # 4. Use the VSGAN instance (with its loaded model) on a clip clip = vsgan.run(clip)  # (optional) Convert back to any other color space if you wish. # clip = core.resize.Point(clip, format=vs.YUV420P8)  # ...  # Don't forget to set the output clip clip.set_output()  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/quick-start/",
        "teaser": null
      },{
        "title": "Models",
        "excerpt":"A PyTorch model file (.pth) is used to initialize the ESRGAN model’s parameters to what it was when training finished or ended. These parameters influence the output.   VSGAN is not an ESRGAN model trainer, it’s a model tester. You will need to provide your own model file for VSGAN to use.   ESRGAN model files have two different state dictionary versions, the older (aka old-arch) version, and the newer/current (aka new-arch) version. The only difference is the naming of the state dict keys. VSGAN automatically supports both versions by renaming new-arch model keys as old-arch keys, detects model scales, and supports any scale.   Need a model file? You can find models on the Game Upscale Discord or their Upscale.wiki Model Database.   Want to train your own? See BasicSR by xinntao which is a model trainer with support for ESRGAN and other architectures. BasicSR trains the previously explained new-arch models (since after commit 9bbc011), and as stated there’s no differences on the model file other than the naming of keys. However, for it’s training code it does have a difference, it restricts model training scale to 4x. If you want to use a different scale, or simply want to train old-arch models, you can use the popular Victorca25 fork.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/models/",
        "teaser": null
      },{
        "title": "VSGAN(device: (int, str) = \"cuda\")",
        "excerpt":"Initializes VSGAN and a PyTorch Device instance.      device (int, str): PyTorch Device identifier. E.g. “cpu”, “cuda”, “cuda:0”, “cuda:1”, 0, 1.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/vsgan-class/",
        "teaser": null
      },{
        "title": "VSGAN.load_model(model: str)",
        "excerpt":"Load an ESRGAN model file into the VSGAN instance. This function can be executed at any point to switch out the loaded model.      model (str): Path to an ESRGAN model file (.pth).  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/vsgan-load-model/",
        "teaser": null
      },{
        "title": "VSGAN.run(clip: VideoNode, chunk: bool = False)",
        "excerpt":"Executes ESRGAN on each frame of the provided clip, returning as a new clip.      clip (VideoNode): VapourSynth clip (VideoNode) to use.   chunk (bool): If your system is running out of memory, try enable this as it will split the image into smaller sub-images and render them one by one, then finally merging them back together. Trading memory requirements for speed and accuracy. WARNING: Since the images will be processed separately, the result may have issues on the edges of the chunks, an example of this issue.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/vsgan-run/",
        "teaser": null
      },{
        "title": "VSGAN.execute(n: int, clip: VideoNode)",
        "excerpt":"Executes the ESRGAN model on nth frame from clip. This function is mainly intended to be used internally. It isn’t an error to use this function, but perhaps you’re looking for VSGAN.run.      n (int): Frame number.   clip (VideoNode): Clip to get the frame from.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/vsgan-execute/",
        "teaser": null
      },{
        "title": "License",
        "excerpt":"The MIT License (MIT)   Copyright (c) 2019-2021 PHOENiX   Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:   The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.   THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.   Documentation uses the Minimal Mistakes Jekyll theme (MIT).   RRDBNet with some small modifications (mainly just the formatting) and ESRGAN related comparison and statistic images used from ESRGAN by xinntao (Apache 2.0).  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/license/",
        "teaser": null
      },]
